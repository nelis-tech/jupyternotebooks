{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "B3g1RG85DdrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install ipywidgets\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "Zn2sfx_DmHLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn \n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "fkq-JGOZmg-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 45:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size=1\n",
        "model = AutoModelForCausalLM.from_pretrained(\"FrostAura/gpt-neo-1.3B-fiction-novel-generation\")\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FrostAura/gpt-neo-1.3B-fiction-novel-generation\")"
      ],
      "metadata": {
        "id": "dyH4oSiFmGZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Generate a novel about hearing voices\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=1000,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(gen_text)"
      ],
      "metadata": {
        "id": "0jUowXy5G2Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    unconditional = False\n",
        "    num_samples = 10\n",
        "    sample_input_file = \"sample_input.txt\"\n",
        "    eval_results_prefix = \"Shakespeare is telling\"\n",
        "    prompt = \"Generate a novel about hearing voices\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    generated_ids = model.generate(input_ids, do_sample=True, max_length=250, temperature=0.9)\n",
        "    generated_text = tokenizer.batch_decode(generated_ids)\n",
        "    print(generated_text)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "81PFEp2WmINb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create output directory if needed\n",
        "\n",
        "if not os.path.exists(\"output_dir\"):\n",
        "\n",
        "    os.makedirs(\"output_dir\")\n",
        "\n",
        "#logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "\n",
        "print(\"Saving model checkpoint to %s\" % \"output_dir\")\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "\n",
        "model_to_save.save_pretrained(\"output_dir\")\n",
        "\n",
        "tokenizer.save_pretrained(\"output_dir\")\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "metadata": {
        "id": "zS71UXH_zOLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    unconditional = False\n",
        "    num_samples = 10\n",
        "    sample_input_file = \"sample_input.txt\"\n",
        "    eval_results_prefix = \"Shakespeare is telling\"\n",
        "    tokenizer = GPTNeoXTokenizer.from_pretrained(\"gpt-neox-20b\")\n",
        "    config = GPTNeoXConfig(is_decoder=True)\n",
        "    model = GPTNeoXForCausalLM.from_pretrained(\"gpt-neox-20b\", config=config)\n",
        "    prompt = \"Generate a novel about hearing voices\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch_device)\n",
        " \n",
        "    generated_ids = model.generate(input_ids, do_sample=True, max_length=250, temperature=0.9)\n",
        "    generated_text = tokenizer.batch_decode(generated_ids)\n",
        "    generated_text_string = \" \".join(\n",
        "        [s.strip() for idx, s in enumerate(generated_text) if idx < 2000&#93;\n",
        "    )\n",
        "    if unconditional:\n",
        "        generated_text_string = &#91;s.strip() for s in generated_text_string&#91;\n",
        "            :3\n",
        "        &#93; + generated_text_string&#91;-2:&#93;&#93;\n",
        "    else:\n",
        "        generated_text_string = &#91;s.strip() for s in generated_text_string&#91;\n",
        "            :3\n",
        "        &#93; + \"NOMORSO_CHIUSA\" + generated_text_string&#91;-2:&#93;&#93;\n",
        " \n",
        "    print(generated_text_string)\n",
        " \n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KBklKj2SoTU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gby-jcbjljY9"
      },
      "outputs": [],
      "source": [
        "\n",
        "NeoXArgsTextgen\n",
        "  \n",
        "# `unconditional`: samples\n",
        " num_samples=10,\n",
        "   \n",
        "# input file\n",
        "sample_input_file = \"sample_input.txt\",\n",
        "  \n",
        "#prefix\n",
        "eval_results_prefix=\"Shakespeare is telling\"\n",
        "\n",
        "prompt = \"Generate a novel about hearing voices\"\n",
        "       \n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "   \n",
        "generated_ids = model.generate(input_ids, do_sample=True, max_length=250, temperature=0.9)\n",
        "generated_text = tokenizer.batch_decode(generated_ids)[0]\n",
        "   \n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        ">>> generate_novel(\n",
        "...     tokenizer=tokenizer,\n",
        "...     model=bert_transformer_tiny_finetuned_sst2_random,\n",
        "...     sequence_length=300,\n",
        "...     do_sample=True,\n",
        "...     temperature=0.8,\n",
        "...     start_token=tokenizer.bos_token,\n",
        "...     end_token=tokenizer.eos_token,\n",
        "...     max_length=250,\n",
        "...     num_samples=10\n",
        "... )\n",
        " \n",
        "        >>> from transformers import BertTokenizer\n",
        "        >>> from transformers import BertTransformer\n",
        "        >>> tokenizer = BertTokenizer.from_pretrained('bert-base-v7')\n",
        "        >>> model = BertTransformer.from_pretrained('bert-base-v7')\n",
        "        >>> sequence_length = 300  # Default"
      ],
      "metadata": {
        "id": "F3TSGLvPo0Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SuNJgURPo1Ux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}