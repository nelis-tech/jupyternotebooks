{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXUUIPYGng-H"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIktr7oknhrN"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 40:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAGsakfdmz1n"
      },
      "outputs": [],
      "source": [
        "!pip3 install aitextgen\n",
        "!pip install datasets\n",
        "!pip install ipywidgets\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iXmnWQjnoPH"
      },
      "outputs": [],
      "source": [
        "from aitextgen import aitextgen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MoNMTJJn94W"
      },
      "outputs": [],
      "source": [
        "model = \"DrishtiSharma/StableDiffusion-Prompt-Generator-GPT-Neo-125M\"\n",
        "tokenizer = \"DrishtiSharma/StableDiffusion-Prompt-Generator-GPT-Neo-125M\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEygreUbn2XW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "ai2 = aitextgen(model=model,\n",
        "                tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J26voqGen-Vv"
      },
      "outputs": [],
      "source": [
        "ai2.generate(prompt=\"\",\n",
        "             n=4,\n",
        "             max_length=500,\n",
        "             temperature=0.6,\n",
        "             )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}